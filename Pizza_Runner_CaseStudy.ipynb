{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c125f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec73e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5606ed19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\bigdatasetup\\spark\\python\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext \n",
    "sc = SparkContext(\"local\", \"App Name\")\n",
    "sql = SQLContext(sc)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "162ed1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Pizza').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06365a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "368ac229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a82427",
   "metadata": {},
   "source": [
    "# df - Runners Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "304f48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"runner_id\", IntegerType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f817e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, '2021-01-01'),\n",
    "        (2, '2021-01-03'),\n",
    "        (3, '2021-01-08'),\n",
    "        (4, '2021-01-15')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af2c4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e67a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- runner_id: integer (nullable = true)\n",
      " |-- registration_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"registration_date\", to_date(\"registration_date\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d81354",
   "metadata": {},
   "source": [
    "# Runners Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4667ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|runner_id|registration_date|\n",
      "+---------+-----------------+\n",
      "|        1|       2021-01-01|\n",
      "|        2|       2021-01-03|\n",
      "|        3|       2021-01-08|\n",
      "|        4|       2021-01-15|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f738c63",
   "metadata": {},
   "source": [
    "# df1 - customer_orders Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b4452da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 =[('1', '101', '1', '', '', '2020-01-01 18:05:02'),\n",
    "  ('2', '101', '1', '', '', '2020-01-01 19:00:52'),\n",
    "  ('3', '102', '1', '', '', '2020-01-02 23:51:23'),\n",
    "  ('3', '102', '2', '','null', '2020-01-02 23:51:23'),\n",
    "  ('4', '103', '1', '4', '', '2020-01-04 13:23:46'),\n",
    "  ('4', '103', '1', '4', '', '2020-01-04 13:23:46'),\n",
    "  ('4', '103', '2', '4', '', '2020-01-04 13:23:46'),\n",
    "  ('5', '104', '1', 'null', '1', '2020-01-08 21:00:29'),\n",
    "  ('6', '101', '2', 'null', 'null', '2020-01-08 21:03:13'),\n",
    "  ('7', '105', '2', 'null', '1', '2020-01-08 21:20:29'),\n",
    "  ('8', '102', '1', 'null', 'null', '2020-01-09 23:54:33'),\n",
    "  ('9', '103', '1', '4', '1, 5', '2020-01-10 11:22:59'),\n",
    "  ('10', '104', '1', 'null', 'null', '2020-01-11 18:34:49'),\n",
    "  ('10', '104', '1', '2, 6', '1, 4', '2020-01-11 18:34:49')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34353cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\",StringType(), True),\n",
    "    StructField(\"pizza_id\",StringType(), True),\n",
    "    StructField(\"exclusions\", StringType(), True),\n",
    "    StructField(\"extras\", StringType(), True),\n",
    "    StructField(\"order_time\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb4f324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = false)\n",
      " |-- customer_id: string (nullable = false)\n",
      " |-- pizza_id: string (nullable = false)\n",
      " |-- exclusions: string (nullable = false)\n",
      " |-- extras: string (nullable = false)\n",
      " |-- order_time: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df1= df1.na.fill(\"0\")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9acaa9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "df1 = df1.withColumn(\"order_id\", col(\"order_id\").cast(\"int\"))\n",
    "df1 = df1.withColumn(\"customer_id\", col(\"customer_id\").cast(\"int\"))\n",
    "df1 = df1.withColumn(\"pizza_id\", col(\"pizza_id\").cast(\"int\"))\n",
    "df1 = df1.withColumn(\"order_time\", to_timestamp(\"order_time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0403f2",
   "metadata": {},
   "source": [
    "# customer_orders Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8aa8521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- pizza_id: integer (nullable = true)\n",
      " |-- exclusions: string (nullable = false)\n",
      " |-- extras: string (nullable = false)\n",
      " |-- order_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f843b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1= df1.na.fill(\"\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d186a6",
   "metadata": {},
   "source": [
    "# df2 - runner_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5db40aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------------+--------+----------+--------------------+\n",
      "|order_id|runner_id|        pickup_time|distance|  duration|        cancellation|\n",
      "+--------+---------+-------------------+--------+----------+--------------------+\n",
      "|       1|        1|2020-01-01 18:15:34|    20km|32 minutes|                    |\n",
      "|       2|        1|2020-01-01 19:10:54|    20km|27 minutes|                    |\n",
      "|       3|        1|2020-01-03 00:12:37|  13.4km|   20 mins|                null|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|        40|                null|\n",
      "|       5|        3|2020-01-08 21:10:57|      10|        15|                null|\n",
      "|       6|        3|               null|    null|      null|Restaurant Cancel...|\n",
      "|       7|        2|2020-01-08 21:30:45|    25km|    25mins|                null|\n",
      "|       8|        2|2020-01-10 00:15:02| 23.4 km| 15 minute|                null|\n",
      "|       9|        2|               null|    null|      null|Customer Cancella...|\n",
      "|      10|        1|2020-01-11 18:50:20|    10km| 10minutes|                null|\n",
      "+--------+---------+-------------------+--------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "data2 = [('1', '1', '2020-01-01 18:15:34', '20km', '32 minutes', ''),\n",
    "  ('2', '1', '2020-01-01 19:10:54', '20km', '27 minutes', ''),\n",
    "  ('3', '1', '2020-01-03 00:12:37', '13.4km', '20 mins', None),\n",
    "  ('4', '2', '2020-01-04 13:53:03', '23.4', '40', None),\n",
    "  ('5', '3', '2020-01-08 21:10:57', '10', '15', None),\n",
    "  ('6', '3', None, None, None, 'Restaurant Cancellation'),\n",
    "  ('7', '2', '2020-01-08 21:30:45', '25km', '25mins', 'null'),\n",
    "  ('8', '2', '2020-01-10 00:15:02', '23.4 km', '15 minute', 'null'),\n",
    "  ('9', '2', None, None, None, 'Customer Cancellation'),\n",
    "  ('10', '1', '2020-01-11 18:50:20', '10km', '10minutes', 'null')]\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"runner_id\", StringType(), True),\n",
    "    StructField(\"pickup_time\", StringType(), True),\n",
    "    StructField(\"distance\", StringType(), True),\n",
    "    StructField(\"duration\", StringType(), True),\n",
    "    StructField(\"cancellation\", StringType(), True)\n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame(data2, schema=schema2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ffeb565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------------+--------+----------+--------------------+\n",
      "|order_id|runner_id|        pickup_time|distance|  duration|        cancellation|\n",
      "+--------+---------+-------------------+--------+----------+--------------------+\n",
      "|       1|        1|2020-01-01 18:15:34|    20km|32 minutes|                    |\n",
      "|       2|        1|2020-01-01 19:10:54|    20km|27 minutes|                    |\n",
      "|       3|        1|2020-01-03 00:12:37|  13.4km|   20 mins|                null|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|        40|                null|\n",
      "|       5|        3|2020-01-08 21:10:57|      10|        15|                null|\n",
      "|       6|        3|               null|    null|      null|Restaurant Cancel...|\n",
      "|       7|        2|2020-01-08 21:30:45|    25km|    25mins|                null|\n",
      "|       8|        2|2020-01-10 00:15:02| 23.4 km| 15 minute|                null|\n",
      "|       9|        2|               null|    null|      null|Customer Cancella...|\n",
      "|      10|        1|2020-01-11 18:50:20|    10km| 10minutes|                null|\n",
      "+--------+---------+-------------------+--------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b57e3",
   "metadata": {},
   "source": [
    "# runner_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ad31eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "|order_id|runner_id|        pickup_time|distance|duration|        cancellation|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "|       1|        1|2020-01-01 18:15:34|      20|     32 |                    |\n",
      "|       2|        1|2020-01-01 19:10:54|      20|     27 |                    |\n",
      "|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                null|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|      40|                null|\n",
      "|       5|        3|2020-01-08 21:10:57|      10|      15|                null|\n",
      "|       6|        3|               null|    null|    null|Restaurant Cancel...|\n",
      "|       7|        2|2020-01-08 21:30:45|      25|      25|                null|\n",
      "|       8|        2|2020-01-10 00:15:02|   23.4 |     15 |                null|\n",
      "|       9|        2|               null|    null|    null|Customer Cancella...|\n",
      "|      10|        1|2020-01-11 18:50:20|      10|      10|                null|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- runner_id: string (nullable = true)\n",
      " |-- pickup_time: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- cancellation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim\n",
    "df2 = df2.selectExpr(\n",
    "    \"order_id\",\n",
    "    \"runner_id\",\n",
    "    \"pickup_time\",\n",
    "    \"CASE WHEN distance LIKE '%km' THEN TRIM(BOTH 'km' FROM distance) ELSE distance END AS distance\",\n",
    "    \"CASE WHEN duration LIKE '%minutes' THEN TRIM(BOTH 'minutes' FROM duration) \"\n",
    "    \"WHEN duration LIKE '%mins' THEN TRIM(BOTH 'mins' FROM duration) \"\n",
    "    \"WHEN duration LIKE '%minute' THEN TRIM(BOTH 'minute' FROM duration) ELSE duration END AS duration\",\n",
    "    \"cancellation\"\n",
    ")\n",
    "\n",
    "\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b321482",
   "metadata": {},
   "source": [
    "# pizza_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce892277",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = [(1, 'Meatlovers'), (2, 'Vegetarian')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd783734",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.createDataFrame(data3, schema=[\"pizza_id\", \"pizza_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5361c15a",
   "metadata": {},
   "source": [
    "# pizza_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30499ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|pizza_id|pizza_name|\n",
      "+--------+----------+\n",
      "|       1|Meatlovers|\n",
      "|       2|Vegetarian|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fab949",
   "metadata": {},
   "source": [
    "# pizza_recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71c47843",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4= [(1, '1, 2, 3, 4, 5, 6, 8, 10'),\n",
    "  (2, '4, 6, 7, 9, 11, 12')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16ce5ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|pizza_id|            toppings|\n",
      "+--------+--------------------+\n",
      "|       1|1, 2, 3, 4, 5, 6,...|\n",
      "|       2|  4, 6, 7, 9, 11, 12|\n",
      "+--------+--------------------+\n",
      "\n",
      "root\n",
      " |-- pizza_id: long (nullable = true)\n",
      " |-- toppings: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.createDataFrame(data4, schema=[\"pizza_id\", \"toppings\"])\n",
    "df4.show()\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b91b6",
   "metadata": {},
   "source": [
    "# df5 pizza_toppings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b36bcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = [\n",
    "    (1, 'Bacon'),\n",
    "    (2, 'BBQ Sauce'),\n",
    "    (3, 'Beef'),\n",
    "    (4, 'Cheese'),\n",
    "    (5, 'Chicken'),\n",
    "    (6, 'Mushrooms'),\n",
    "    (7, 'Onions'),\n",
    "    (8, 'Pepperoni'),\n",
    "    (9, 'Peppers'),\n",
    "    (10, 'Salami'),\n",
    "    (11, 'Tomatoes'),\n",
    "    (12, 'Tomato Sauce')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de9c86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = spark.createDataFrame(data5, [\"topping_id\", \"topping_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decaade4",
   "metadata": {},
   "source": [
    "# pizza_toppings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "674c8436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|topping_id|topping_name|\n",
      "+----------+------------+\n",
      "|         1|       Bacon|\n",
      "|         2|   BBQ Sauce|\n",
      "|         3|        Beef|\n",
      "|         4|      Cheese|\n",
      "|         5|     Chicken|\n",
      "|         6|   Mushrooms|\n",
      "|         7|      Onions|\n",
      "|         8|   Pepperoni|\n",
      "|         9|     Peppers|\n",
      "|        10|      Salami|\n",
      "|        11|    Tomatoes|\n",
      "|        12|Tomato Sauce|\n",
      "+----------+------------+\n",
      "\n",
      "root\n",
      " |-- topping_id: long (nullable = true)\n",
      " |-- topping_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()\n",
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4f006",
   "metadata": {},
   "source": [
    "# 1. How many pizzas were ordered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09d2ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d7d4581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "Total_PZ_Odr = df1.select(count('order_id')).first()[0]\n",
    "print(Total_PZ_Odr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3774ec81",
   "metadata": {},
   "source": [
    "# 2. How many unique customer orders were made?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3baa6f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "Total_Orders = df1.select(approx_count_distinct('order_id')).first()[0]\n",
    "print(Total_Orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270c2ec",
   "metadata": {},
   "source": [
    "# 3. How many successful orders were delivered by each runner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05dc9e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "|order_id|runner_id|        pickup_time|distance|duration|        cancellation|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "|       1|        1|2020-01-01 18:15:34|      20|     32 |                    |\n",
      "|       2|        1|2020-01-01 19:10:54|      20|     27 |                    |\n",
      "|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                null|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|      40|                null|\n",
      "|       5|        3|2020-01-08 21:10:57|      10|      15|                null|\n",
      "|       6|        3|               null|    null|    null|Restaurant Cancel...|\n",
      "|       7|        2|2020-01-08 21:30:45|      25|      25|                null|\n",
      "|       8|        2|2020-01-10 00:15:02|   23.4 |     15 |                null|\n",
      "|       9|        2|               null|    null|    null|Customer Cancella...|\n",
      "|      10|        1|2020-01-11 18:50:20|      10|      10|                null|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "167c876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|runner_id|OrdersDelivered|\n",
      "+---------+---------------+\n",
      "|        1|              4|\n",
      "|        2|              3|\n",
      "|        3|              1|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df2.select('runner_id', 'order_id').filter(df2.distance.isNotNull()).groupBy('runner_id') \\\n",
    "   .agg(count('order_id').alias('OrdersDelivered')).sort(\"runner_id\") \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c2d65",
   "metadata": {},
   "source": [
    "# 4. How many of each type of pizza was delivered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e48763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|pizza_id|pizza_name|\n",
      "+--------+----------+\n",
      "|       1|Meatlovers|\n",
      "|       2|Vegetarian|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f41387d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+--------+---------+-------------------+--------+--------+--------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|order_id|runner_id|        pickup_time|distance|duration|        cancellation|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+---------+-------------------+--------+--------+--------------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|       1|        1|2020-01-01 18:15:34|      20|     32 |                    |\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|       2|        1|2020-01-01 19:10:54|      20|     27 |                    |\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                null|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                null|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|       4|        2|2020-01-04 13:53:03|    23.4|      40|                null|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|       4|        2|2020-01-04 13:53:03|    23.4|      40|                null|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|       4|        2|2020-01-04 13:53:03|    23.4|      40|                null|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|       5|        3|2020-01-08 21:10:57|      10|      15|                null|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|       6|        3|               null|    null|    null|Restaurant Cancel...|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|       7|        2|2020-01-08 21:30:45|      25|      25|                null|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|       8|        2|2020-01-10 00:15:02|   23.4 |     15 |                null|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|       9|        2|               null|    null|    null|Customer Cancella...|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|      10|        1|2020-01-11 18:50:20|      10|      10|                null|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|      10|        1|2020-01-11 18:50:20|      10|      10|                null|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+---------+-------------------+--------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Del = df1.join(df2, df1.order_id == df2.order_id,how = 'inner')\n",
    "Del.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "842fbf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+--------+---------+-------------------+--------+--------+--------------------+----------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|order_id|runner_id|        pickup_time|distance|duration|        cancellation|pizza_name|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+---------+-------------------+--------+--------+--------------------+----------+\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|      10|        1|2020-01-11 18:50:20|      10|      10|                null|Meatlovers|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|      10|        1|2020-01-11 18:50:20|      10|      10|                null|Meatlovers|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|       9|        2|               null|    null|    null|Customer Cancella...|Meatlovers|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|       8|        2|2020-01-10 00:15:02|   23.4 |     15 |                null|Meatlovers|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|       5|        3|2020-01-08 21:10:57|      10|      15|                null|Meatlovers|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|       4|        2|2020-01-04 13:53:03|    23.4|      40|                null|Meatlovers|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|       4|        2|2020-01-04 13:53:03|    23.4|      40|                null|Meatlovers|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                null|Meatlovers|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|       2|        1|2020-01-01 19:10:54|      20|     27 |                    |Meatlovers|\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|       1|        1|2020-01-01 18:15:34|      20|     32 |                    |Meatlovers|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|       7|        2|2020-01-08 21:30:45|      25|      25|                null|Vegetarian|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|       6|        3|               null|    null|    null|Restaurant Cancel...|Vegetarian|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|       4|        2|2020-01-04 13:53:03|    23.4|      40|                null|Vegetarian|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                null|Vegetarian|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+---------+-------------------+--------+--------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Del1 = Del.join(df3, Del.pizza_id == df3.pizza_id,how = 'inner').select(Del[\"*\"],df3[\"pizza_name\"])\n",
    "Del1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7587cb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|pizza_name|TotalDelivered|\n",
      "+----------+--------------+\n",
      "|Vegetarian|             3|\n",
      "|Meatlovers|             9|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "\n",
    "Del1.filter(df2.distance.isNotNull()) \\\n",
    "   .groupBy('pizza_name') \\\n",
    "   .agg(count('pizza_id').alias('TotalDelivered')) \\\n",
    "   .sort('TotalDelivered') \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd3e77",
   "metadata": {},
   "source": [
    "# 5. How many Vegetarian and Meatlovers were ordered by each customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df35ead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------------+\n",
      "|customer_id|pizza_name|Total_PZ_Ordered|\n",
      "+-----------+----------+----------------+\n",
      "|        101|Meatlovers|               2|\n",
      "|        101|Vegetarian|               1|\n",
      "|        102|Vegetarian|               1|\n",
      "|        102|Meatlovers|               2|\n",
      "|        103|Meatlovers|               3|\n",
      "|        103|Vegetarian|               1|\n",
      "|        104|Meatlovers|               3|\n",
      "|        105|Vegetarian|               1|\n",
      "+-----------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "\n",
    "Del1.groupBy('customer_id','pizza_name') \\\n",
    "   .agg(count('pizza_id').alias('Total_PZ_Ordered')) \\\n",
    "   .sort('customer_id') \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd079e",
   "metadata": {},
   "source": [
    "# 6. What was the maximum number of pizzas delivered in a single order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dda44357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------------+--------+--------+--------------------+--------+\n",
      "|order_id|runner_id|        pickup_time|distance|duration|        cancellation|pizza_id|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+--------+\n",
      "|       1|        1|2020-01-01 18:15:34|      20|     32 |                    |       1|\n",
      "|       2|        1|2020-01-01 19:10:54|      20|     27 |                    |       1|\n",
      "|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                null|       1|\n",
      "|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                null|       2|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|      40|                null|       1|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|      40|                null|       1|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|      40|                null|       2|\n",
      "|       5|        3|2020-01-08 21:10:57|      10|      15|                null|       1|\n",
      "|       6|        3|               null|    null|    null|Restaurant Cancel...|       2|\n",
      "|       7|        2|2020-01-08 21:30:45|      25|      25|                null|       2|\n",
      "|       8|        2|2020-01-10 00:15:02|   23.4 |     15 |                null|       1|\n",
      "|       9|        2|               null|    null|    null|Customer Cancella...|       1|\n",
      "|      10|        1|2020-01-11 18:50:20|      10|      10|                null|       1|\n",
      "|      10|        1|2020-01-11 18:50:20|      10|      10|                null|       1|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Del2 = df2.join(df1, df2.order_id == df1.order_id,how = 'inner').select(df2[\"*\"],df1[\"pizza_id\"])\n",
    "Del2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ef91025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|order_id|TotalDelivered|\n",
      "+--------+--------------+\n",
      "|       4|             3|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "Del2.filter(df2.distance.isNotNull()).groupBy('order_id').agg(count(\"pizza_id\").alias(\"TotalDelivered\")).orderBy(desc(\"TotalDelivered\")).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12faaa",
   "metadata": {},
   "source": [
    "### 7. For each customer, how many delivered pizzas had at least 1 change and how many had no changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c4ba4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+--------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|distance|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|      20|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|      20|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|    13.4|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|    13.4|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|    23.4|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|    23.4|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|    23.4|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|      10|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|    null|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|      25|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|   23.4 |\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|    null|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|      10|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|      10|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "JoinDF = df1.join(df2, df1.order_id == df2.order_id, how = 'inner').select(df1[\"*\"],df2[\"distance\"])\n",
    "JoinDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bd44079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+--------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|distance|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+\n",
      "|       1|        101|       1|         0|     0|2020-01-01 18:05:02|      20|\n",
      "|       2|        101|       1|         0|     0|2020-01-01 19:00:52|      20|\n",
      "|       3|        102|       1|         0|     0|2020-01-02 23:51:23|    13.4|\n",
      "|       3|        102|       2|         0|  null|2020-01-02 23:51:23|    13.4|\n",
      "|       4|        103|       1|         4|     0|2020-01-04 13:23:46|    23.4|\n",
      "|       4|        103|       1|         4|     0|2020-01-04 13:23:46|    23.4|\n",
      "|       4|        103|       2|         4|     0|2020-01-04 13:23:46|    23.4|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|      10|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|    null|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|      25|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|   23.4 |\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|    null|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|      10|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|      10|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "JoinDF = JoinDF.withColumn(\"extras\", regexp_replace(\"extras\", \"^\\s*$\", \"0\"))\n",
    "JoinDF = JoinDF.withColumn(\"exclusions\", regexp_replace(\"exclusions\", \"^\\s*$\", \"0\"))\n",
    "JoinDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04de5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, sum\n",
    "\n",
    "JoinDF1 = JoinDF.filter(df2.distance.isNotNull())\\\n",
    "            .select(\"customer_id\", \n",
    "            when((col(\"exclusions\").isNotNull() & (col(\"exclusions\") != \"0\")) | (col(\"extras\").isNotNull() & (col(\"extras\") != \"0\")), 1)\n",
    "            .otherwise(0)\n",
    "            .alias(\"AtleastOneChange\"),\n",
    "            when((col(\"exclusions\").isNull() | (col(\"exclusions\") == \"0\")) & (col(\"extras\").isNull() | (col(\"extras\") == \"0\")), 1)\n",
    "            .otherwise(0)\n",
    "            .alias(\"NoChange\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9886378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+--------+\n",
      "|customer_id|AtleastOneChange|NoChange|\n",
      "+-----------+----------------+--------+\n",
      "|        101|               0|       2|\n",
      "|        103|               3|       0|\n",
      "|        102|               2|       1|\n",
      "|        105|               1|       0|\n",
      "|        104|               3|       0|\n",
      "+-----------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, sum\n",
    "\n",
    "result = JoinDF \\\n",
    "    .filter(df2.distance.isNotNull()) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        sum(\n",
    "            when((col(\"exclusions\").isNotNull() & (col(\"exclusions\") != \"0\")) | (col(\"extras\").isNotNull() & (col(\"extras\") != \"0\")), 1)\n",
    "            .otherwise(0)\n",
    "        ).alias(\"AtleastOneChange\"),\n",
    "        sum(\n",
    "            when((col(\"exclusions\").isNull() | (col(\"exclusions\") == \"0\")) & (col(\"extras\").isNull() | (col(\"extras\") == \"0\")), 1)\n",
    "            .otherwise(0)\n",
    "        ).alias(\"NoChange\")\n",
    "    )\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "875155e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+--------+\n",
      "|customer_id|AtleastOneChange|NoChange|\n",
      "+-----------+----------------+--------+\n",
      "|        101|               0|       2|\n",
      "|        103|               3|       0|\n",
      "|        102|               2|       1|\n",
      "|        105|               1|       0|\n",
      "|        104|               3|       0|\n",
      "+-----------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Joindf_grouped = JoinDF1.groupBy(\"customer_id\").agg(sum(\"AtleastOneChange\").alias(\"AtleastOneChange\"), sum(\"NoChange\").alias(\"NoChange\"))\n",
    "Joindf_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09589f09",
   "metadata": {},
   "source": [
    "# 8. How many pizzas were delivered that had both exclusions and extras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "226ef55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+--------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|distance|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+\n",
      "|       1|        101|       1|         0|     0|2020-01-01 18:05:02|      20|\n",
      "|       2|        101|       1|         0|     0|2020-01-01 19:00:52|      20|\n",
      "|       3|        102|       1|         0|     0|2020-01-02 23:51:23|    13.4|\n",
      "|       3|        102|       2|         0|  null|2020-01-02 23:51:23|    13.4|\n",
      "|       4|        103|       1|         4|     0|2020-01-04 13:23:46|    23.4|\n",
      "|       4|        103|       1|         4|     0|2020-01-04 13:23:46|    23.4|\n",
      "|       4|        103|       2|         4|     0|2020-01-04 13:23:46|    23.4|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|      10|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|    null|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|      25|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|   23.4 |\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|    null|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|      10|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|      10|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- pizza_id: integer (nullable = true)\n",
      " |-- exclusions: string (nullable = false)\n",
      " |-- extras: string (nullable = false)\n",
      " |-- order_time: timestamp (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "JoinDF.show()\n",
    "JoinDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de21aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, sum\n",
    "\n",
    "result1 = JoinDF \\\n",
    "    .filter(col(\"distance\") != 0) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        sum(\n",
    "            when(col(\"exclusions\").isNotNull() & (col(\"exclusions\") != \"0\") & col(\"extras\").isNotNull() & (col(\"extras\") != \"0\"), 1)\n",
    "            .otherwise(0)\n",
    "        ).alias(\"BothExclusionExtra\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"BothExclusionExtra\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcd329e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|customer_id|BothExclusionExtra|\n",
      "+-----------+------------------+\n",
      "|        104|                 3|\n",
      "|        105|                 1|\n",
      "|        102|                 1|\n",
      "|        101|                 0|\n",
      "|        103|                 0|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b60c31",
   "metadata": {},
   "source": [
    "# 9.What was the total volume of pizzas ordered for each hour of the day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8db7faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|Hour_data|Total_Pizza_Ordered|\n",
      "+---------+-------------------+\n",
      "|       11|                  1|\n",
      "|       13|                  3|\n",
      "|       18|                  3|\n",
      "|       19|                  1|\n",
      "|       21|                  3|\n",
      "|       23|                  3|\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, hour\n",
    "\n",
    "Total_Vol = JoinDF \\\n",
    "    .select(hour(\"order_time\").alias(\"Hour_data\"), col(\"order_id\")) \\\n",
    "    .groupBy(\"Hour_data\") \\\n",
    "    .agg(count(\"order_id\").alias(\"Total_Pizza_Ordered\")) \\\n",
    "    .orderBy(\"Hour_data\")\n",
    "\n",
    "Total_Vol.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69a62c",
   "metadata": {},
   "source": [
    "# 10. What was the volume of orders for each day of the week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33de39b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "| Day_Data|Total_Pizza_Ordered|\n",
      "+---------+-------------------+\n",
      "|Wednesday|                  5|\n",
      "| Saturday|                  5|\n",
      "| Thursday|                  3|\n",
      "|   Friday|                  1|\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, dayofweek, date_format\n",
    "Vol_Orders_Day = JoinDF \\\n",
    "    .select(date_format(\"order_time\",'EEEE').alias(\"Day_Data\"), col(\"order_id\")) \\\n",
    "    .groupBy(\"Day_Data\") \\\n",
    "    .agg(count(\"order_id\").alias(\"Total_Pizza_Ordered\")) \\\n",
    "    .orderBy(col(\"Total_Pizza_Ordered\").desc())\n",
    "\n",
    "Vol_Orders_Day.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15a4b8",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7efa301",
   "metadata": {},
   "source": [
    "### 1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38a8726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|runner_id|registration_date|\n",
      "+---------+-----------------+\n",
      "|        1|       2021-01-01|\n",
      "|        2|       2021-01-03|\n",
      "|        3|       2021-01-08|\n",
      "|        4|       2021-01-15|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0a4df49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|         week_start|number_runners|\n",
      "+-------------------+--------------+\n",
      "|2020-12-28 00:00:00|             2|\n",
      "|2021-01-04 00:00:00|             1|\n",
      "|2021-01-11 00:00:00|             1|\n",
      "+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, dayofweek, date_format\n",
    "from pyspark.sql.functions import date_trunc, countDistinct\n",
    "\n",
    "Run_SingedUP1 = df \\\n",
    "              .select(date_trunc(\"week\", \"registration_date\").alias(\"week_start\"), col(\"runner_id\")) \\\n",
    "              .groupBy(\"week_start\") \\\n",
    "              .agg(countDistinct(\"runner_id\").alias(\"number_runners\")) \\\n",
    "              .orderBy(\"week_start\")\n",
    "Run_SingedUP1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a1dc1",
   "metadata": {},
   "source": [
    "#### 2. What was the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pickup the order ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "643e4f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------------+--------+--------+--------------------+-------------------+\n",
      "|order_id|runner_id|        pickup_time|distance|duration|        cancellation|         order_time|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+-------------------+\n",
      "|       1|        1|2020-01-01 18:15:34|      20|     32 |                    |2020-01-01 18:05:02|\n",
      "|       2|        1|2020-01-01 19:10:54|      20|     27 |                    |2020-01-01 19:00:52|\n",
      "|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                   0|2020-01-02 23:51:23|\n",
      "|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                   0|2020-01-02 23:51:23|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|      40|                   0|2020-01-04 13:23:46|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|      40|                   0|2020-01-04 13:23:46|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|      40|                   0|2020-01-04 13:23:46|\n",
      "|       5|        3|2020-01-08 21:10:57|      10|      15|                   0|2020-01-08 21:00:29|\n",
      "|       6|        3|                  0|       0|       0|Restaurant Cancel...|2020-01-08 21:03:13|\n",
      "|       7|        2|2020-01-08 21:30:45|      25|      25|                null|2020-01-08 21:20:29|\n",
      "|       8|        2|2020-01-10 00:15:02|   23.4 |     15 |                null|2020-01-09 23:54:33|\n",
      "|       9|        2|                  0|       0|       0|Customer Cancella...|2020-01-10 11:22:59|\n",
      "|      10|        1|2020-01-11 18:50:20|      10|      10|                null|2020-01-11 18:34:49|\n",
      "|      10|        1|2020-01-11 18:50:20|      10|      10|                null|2020-01-11 18:34:49|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Part2 = df2.join(df1, df2.order_id == df1.order_id,how = 'inner').select(df2[\"*\"],df1[\"order_time\"])\n",
    "Part2 = Part2.fillna(\"0\")\n",
    "Part2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eaf5687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|runner_id|AvgTime|\n",
      "+---------+-------+\n",
      "|        3|   10.0|\n",
      "|        1|  15.33|\n",
      "|        2|   23.4|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, round, expr\n",
    "\n",
    "AVg = Part2 \\\n",
    "    .groupBy(\"runner_id\") \\\n",
    "    .agg(round(avg(expr(\"timestampdiff(minute, order_time, pickup_time)\")), 2).alias(\"AvgTime\")) \\\n",
    "    .orderBy(\"AvgTime\")\n",
    "\n",
    "AVg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248dafc",
   "metadata": {},
   "source": [
    "#### 3. Is there any relationship between the number of pizzas and how long the order takes to prepare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57350551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+\n",
      "|order_id|PizzaCount|AvgTime|\n",
      "+--------+----------+-------+\n",
      "|       7|         1|   10.0|\n",
      "|       5|         1|   10.0|\n",
      "|       1|         1|   10.0|\n",
      "|       2|         1|   10.0|\n",
      "|      10|         2|   15.0|\n",
      "|       8|         1|   20.0|\n",
      "|       3|         2|   21.0|\n",
      "|       4|         3|   29.0|\n",
      "+--------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, round, expr\n",
    "\n",
    "Rn = Part2 \\\n",
    "    .filter(col(\"distance\") != \"0\") \\\n",
    "    .groupBy(\"order_id\") \\\n",
    "    .agg(count(\"order_id\").alias(\"PizzaCount\"),round(avg(expr(\"timestampdiff(minute, order_time, pickup_time)\")), 2).alias(\"AvgTime\")) \\\n",
    "    .orderBy(\"AvgTime\")\n",
    "Rn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8259a63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|PizzaCount|Avgtime|\n",
      "+----------+-------+\n",
      "|         1|   10.0|\n",
      "|         2|   15.0|\n",
      "|         3|   29.0|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, round, expr\n",
    "\n",
    "Rn = Part2 \\\n",
    "    .filter(col(\"distance\") != \"0\") \\\n",
    "    .groupBy(\"order_id\") \\\n",
    "    .agg(count(\"order_id\").alias(\"PizzaCount\"),round(avg(expr(\"timestampdiff(minute, order_time, pickup_time)\")), 2).alias(\"AvgTime\")) \\\n",
    "    .orderBy(\"AvgTime\")\n",
    "\n",
    "Rn1 = Rn \\\n",
    "    .groupBy(\"PizzaCount\") \\\n",
    "    .agg(expr(\"first(Avgtime)\").alias(\"Avgtime\")) \\\n",
    "    .orderBy(\"PizzaCount\")\n",
    "\n",
    "Rn1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3374545",
   "metadata": {},
   "source": [
    "# 4. What was the average distance travelled for each customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61999e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+--------+-------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|distance|        pickup_time|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+-------------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|      20|2020-01-01 18:15:34|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|      20|2020-01-01 19:10:54|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|    13.4|2020-01-03 00:12:37|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|    13.4|2020-01-03 00:12:37|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|    23.4|2020-01-04 13:53:03|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|    23.4|2020-01-04 13:53:03|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|    23.4|2020-01-04 13:53:03|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|      10|2020-01-08 21:10:57|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|       0|                  0|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|      25|2020-01-08 21:30:45|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|   23.4 |2020-01-10 00:15:02|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|       0|                  0|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|      10|2020-01-11 18:50:20|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|      10|2020-01-11 18:50:20|\n",
      "+--------+-----------+--------+----------+------+-------------------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "New = df1.join(df2, df1.order_id == df2.order_id,how = 'inner').select(df1[\"*\"],df2[\"distance\"], df2[\"pickup_time\"])\n",
    "New = New.fillna(\"0\")\n",
    "New.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00e12752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|avg_distance|\n",
      "+-----------+------------+\n",
      "|        101|        20.0|\n",
      "|        103|        23.4|\n",
      "|        102|       16.73|\n",
      "|        105|        25.0|\n",
      "|        104|        10.0|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, round\n",
    "\n",
    "AvgDistance = New \\\n",
    "            .filter(col(\"distance\") != \"0\") \\\n",
    "            .groupBy(\"customer_id\") \\\n",
    "            .agg(round(avg(\"distance\"), 2).alias(\"avg_distance\"))\n",
    "\n",
    "AvgDistance.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9fd51",
   "metadata": {},
   "source": [
    "#### 5. What was the difference between the longest and shortest delivery times for all orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90250320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between longest and shortest delivery times: 30 min\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n",
    "# convert duration column to integer type\n",
    "completed_deliveries = df2.withColumn(\"duration_min\", col(\"duration\").cast(\"int\"))\n",
    "\n",
    "# compute min and max durations\n",
    "min_duration = completed_deliveries.agg({\"duration_min\": \"min\"}).collect()[0][0]\n",
    "max_duration = completed_deliveries.agg({\"duration_min\": \"max\"}).collect()[0][0]\n",
    "\n",
    "# calculate difference between max and min durations\n",
    "duration_diff = max_duration - min_duration\n",
    "\n",
    "print(\"Difference between longest and shortest delivery times: {} min\".format(duration_diff))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a7483",
   "metadata": {},
   "source": [
    "### 6. What was the average speed for each runner for each delivery and do you notice any trend for these values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "454d00a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "|order_id|runner_id|        pickup_time|distance|duration|        cancellation|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "|       1|        1|2020-01-01 18:15:34|      20|     32 |                    |\n",
      "|       2|        1|2020-01-01 19:10:54|      20|     27 |                    |\n",
      "|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                   0|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|      40|                   0|\n",
      "|       5|        3|2020-01-08 21:10:57|      10|      15|                   0|\n",
      "|       6|        3|                  0|       0|       0|Restaurant Cancel...|\n",
      "|       7|        2|2020-01-08 21:30:45|      25|      25|                null|\n",
      "|       8|        2|2020-01-10 00:15:02|   23.4 |     15 |                null|\n",
      "|       9|        2|                  0|       0|       0|Customer Cancella...|\n",
      "|      10|        1|2020-01-11 18:50:20|      10|      10|                null|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df21 = df2.fillna(\"0\")\n",
    "df21.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0a7be44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------+\n",
      "|runner_id|order_id|speedKMH|\n",
      "+---------+--------+--------+\n",
      "|        1|       1|    37.5|\n",
      "|        1|       2|    44.4|\n",
      "|        1|      10|    60.0|\n",
      "|        1|       3|    40.2|\n",
      "|        2|       8|    93.6|\n",
      "|        2|       4|    35.1|\n",
      "|        2|       7|    60.0|\n",
      "|        3|       5|    40.0|\n",
      "+---------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "AVG_Speed = df21 \\\n",
    "       .filter(col(\"distance\") != \"0\")\\\n",
    "       .groupBy(\"runner_id\", \"order_id\")\\\n",
    "       .agg(avg(round((col(\"distance\") * 60) / col(\"duration\"), 1)).alias(\"speedKMH\"))\\\n",
    "       .orderBy(\"runner_id\")\\\n",
    "      \n",
    "AVG_Speed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262cbb2",
   "metadata": {},
   "source": [
    "# 7. What is the successful delivery percentage for each runner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89523a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "|order_id|runner_id|        pickup_time|distance|duration|        cancellation|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "|       1|        1|2020-01-01 18:15:34|      20|     32 |                    |\n",
      "|       2|        1|2020-01-01 19:10:54|      20|     27 |                    |\n",
      "|       3|        1|2020-01-03 00:12:37|    13.4|     20 |                   0|\n",
      "|       4|        2|2020-01-04 13:53:03|    23.4|      40|                   0|\n",
      "|       5|        3|2020-01-08 21:10:57|      10|      15|                   0|\n",
      "|       6|        3|                  0|       0|       0|Restaurant Cancel...|\n",
      "|       7|        2|2020-01-08 21:30:45|      25|      25|                null|\n",
      "|       8|        2|2020-01-10 00:15:02|   23.4 |     15 |                null|\n",
      "|       9|        2|                  0|       0|       0|Customer Cancella...|\n",
      "|      10|        1|2020-01-11 18:50:20|      10|      10|                null|\n",
      "+--------+---------+-------------------+--------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df21.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5a702f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|runner_id|Successfulpercentage|\n",
      "+---------+--------------------+\n",
      "|        1|               100.0|\n",
      "|        2|                75.0|\n",
      "|        3|                50.0|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, count, when, round\n",
    "\n",
    "P = df21 \\\n",
    "    .groupBy(\"runner_id\")\\\n",
    "    .agg(sum(when(col(\"distance\") != \"0\", 1)).alias(\"Delivered\"), count(\"order_id\").alias(\"Total_Orders\"))\\\n",
    "      \n",
    "\n",
    "M = P\\\n",
    "    .select(col(\"runner_id\"), round((col(\"Delivered\")/col(\"Total_Orders\"))*100).alias(\"Successfulpercentage\"))\\\n",
    "    .orderBy(\"runner_id\")\\\n",
    "    \n",
    "\n",
    "M.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee05ef0",
   "metadata": {},
   "source": [
    "# C. Ingredient Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd001b6",
   "metadata": {},
   "source": [
    "# 1. What are the standard ingredients for each pizza?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9b8fcec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|pizza_id|pizza_name|\n",
      "+--------+----------+\n",
      "|       1|Meatlovers|\n",
      "|       2|Vegetarian|\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------------------+\n",
      "|pizza_id|            toppings|\n",
      "+--------+--------------------+\n",
      "|       1|1, 2, 3, 4, 5, 6,...|\n",
      "|       2|  4, 6, 7, 9, 11, 12|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8bb4ae59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|pizza_id|pizza_name|            toppings|\n",
      "+--------+----------+--------------------+\n",
      "|       1|Meatlovers|1, 2, 3, 4, 5, 6,...|\n",
      "|       2|Vegetarian|  4, 6, 7, 9, 11, 12|\n",
      "+--------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NewDF = df3.join(df4, df3.pizza_id == df4.pizza_id, how = 'inner').select(df3[\"*\"],df4[\"toppings\"])\n",
    "NewDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20d25171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pizza_id: long (nullable = true)\n",
      " |-- pizza_name: string (nullable = true)\n",
      " |-- toppings: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NewDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f3f479e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "NewDF = NewDF.withColumn(\"New_Topp\", split(NewDF.toppings, \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4748c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+--------------------+\n",
      "|pizza_id|pizza_name|            toppings|            New_Topp|\n",
      "+--------+----------+--------------------+--------------------+\n",
      "|       1|Meatlovers|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       2|Vegetarian|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|\n",
      "+--------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NewDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f66175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|topping_id|topping_name|\n",
      "+----------+------------+\n",
      "|         1|       Bacon|\n",
      "|         2|   BBQ Sauce|\n",
      "|         3|        Beef|\n",
      "|         4|      Cheese|\n",
      "|         5|     Chicken|\n",
      "|         6|   Mushrooms|\n",
      "|         7|      Onions|\n",
      "|         8|   Pepperoni|\n",
      "|         9|     Peppers|\n",
      "|        10|      Salami|\n",
      "|        11|    Tomatoes|\n",
      "|        12|Tomato Sauce|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38d2369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|pizza_id|pizza_name|        all_toppings|\n",
      "+--------+----------+--------------------+\n",
      "|       1|Meatlovers|[Mushrooms, Chick...|\n",
      "|       2|Vegetarian|[Onions, Mushroom...|\n",
      "+--------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "\n",
    "NewDF_exploded = NewDF.select(\"pizza_id\", \"pizza_name\", explode(\"New_Topp\").alias(\"topping_id\"))\n",
    "\n",
    "\n",
    "Output = NewDF_exploded.join(df5, on=\"topping_id\")\n",
    "\n",
    "\n",
    "AF = Output.groupBy(\"pizza_id\", \"pizza_name\").agg(collect_list(\"topping_name\").alias(\"all_toppings\"))\n",
    "AF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4019826",
   "metadata": {},
   "source": [
    "# 2. What was the most commonly added extra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f413eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- pizza_id: integer (nullable = true)\n",
      " |-- exclusions: string (nullable = false)\n",
      " |-- extras: string (nullable = false)\n",
      " |-- order_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "470fda46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_up = df1.filter(df1.extras.isNotNull())\n",
    "df_up.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f27f276e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|extras_array|\n",
      "+--------+-----------+--------+----------+------+-------------------+------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|          []|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|          []|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|          []|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|      [null]|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|          []|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|          []|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|          []|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|         [1]|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|      [null]|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|         [1]|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|      [null]|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|     [1,  5]|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|      [null]|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|     [1,  4]|\n",
      "+--------+-----------+--------+----------+------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df_up = df_up.withColumn(\"extras_array\", split(df_up.extras, \",\"))\n",
    "df_up.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1022efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra = df_up.select(explode(df_up.extras_array).alias(\"extra\")) \\\n",
    "       .groupBy(\"extra\") \\\n",
    "       .agg(count(\"extra\").alias(\"num_times_added\")) \\\n",
    "       .orderBy(\"num_times_added\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eda5384f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|extra|num_times_added|\n",
      "+-----+---------------+\n",
      "|     |              6|\n",
      "| null|              4|\n",
      "|    1|              4|\n",
      "|    4|              1|\n",
      "|    5|              1|\n",
      "+-----+---------------+\n",
      "\n",
      "root\n",
      " |-- extra: string (nullable = false)\n",
      " |-- num_times_added: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_extra.show()\n",
    "df_extra.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640fba12",
   "metadata": {},
   "source": [
    "# 3. What was the most common exclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f01beacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+------------+----------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|extras_array|exclusions_array|\n",
      "+--------+-----------+--------+----------+------+-------------------+------------+----------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|          []|              []|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|          []|              []|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|          []|              []|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|      [null]|              []|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|          []|             [4]|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|          []|             [4]|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|          []|             [4]|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|         [1]|          [null]|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|      [null]|          [null]|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|         [1]|          [null]|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|      [null]|          [null]|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|     [1,  5]|             [4]|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|      [null]|          [null]|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|     [1,  4]|         [2,  6]|\n",
      "+--------+-----------+--------+----------+------+-------------------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df_up1 = df_up.withColumn(\"exclusions_array\", split(df_up.exclusions, \",\"))\n",
    "df_up1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "123f5e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra1 = df_up1.select(explode(df_up1.exclusions_array).alias(\"exclusion\")) \\\n",
    "       .groupBy(\"exclusion\") \\\n",
    "       .agg(count(\"exclusion\").alias(\"num_times_added\")) \\\n",
    "       .orderBy(\"num_times_added\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "04baefe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|exclusion|num_times_added|\n",
      "+---------+---------------+\n",
      "|     null|              5|\n",
      "|         |              4|\n",
      "|        4|              4|\n",
      "|        6|              1|\n",
      "|        2|              1|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_extra1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c8996",
   "metadata": {},
   "source": [
    "##### 4. Generate an order item for each record in the customers_orders table in the format of one of the following:\n",
    "Meat Lovers\n",
    "Meat Lovers - Exclude Beef\n",
    "Meat Lovers - Extra Bacon\n",
    "Meat Lovers - Exclude Cheese, Bacon - Extra Mushroom, Peppers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4add97ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+------------+----------------+--------------------+\n",
      "|order_id|customer_id|pizza_id|extras_array|exclusions_array|            toppings|\n",
      "+--------+-----------+--------+------------+----------------+--------------------+\n",
      "|       1|        101|       1|          []|              []|1, 2, 3, 4, 5, 6,...|\n",
      "|       2|        101|       1|          []|              []|1, 2, 3, 4, 5, 6,...|\n",
      "|       3|        102|       1|          []|              []|1, 2, 3, 4, 5, 6,...|\n",
      "|       4|        103|       1|          []|             [4]|1, 2, 3, 4, 5, 6,...|\n",
      "|       4|        103|       1|          []|             [4]|1, 2, 3, 4, 5, 6,...|\n",
      "|       5|        104|       1|         [1]|          [null]|1, 2, 3, 4, 5, 6,...|\n",
      "|       8|        102|       1|      [null]|          [null]|1, 2, 3, 4, 5, 6,...|\n",
      "|       9|        103|       1|     [1,  5]|             [4]|1, 2, 3, 4, 5, 6,...|\n",
      "|      10|        104|       1|      [null]|          [null]|1, 2, 3, 4, 5, 6,...|\n",
      "|      10|        104|       1|     [1,  4]|         [2,  6]|1, 2, 3, 4, 5, 6,...|\n",
      "|       3|        102|       2|      [null]|              []|  4, 6, 7, 9, 11, 12|\n",
      "|       4|        103|       2|          []|             [4]|  4, 6, 7, 9, 11, 12|\n",
      "|       6|        101|       2|      [null]|          [null]|  4, 6, 7, 9, 11, 12|\n",
      "|       7|        105|       2|         [1]|          [null]|  4, 6, 7, 9, 11, 12|\n",
      "+--------+-----------+--------+------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_co = df_up1.join(df4, df_up1.pizza_id == df4.pizza_id, how = 'inner').select(df_up1[\"order_id\"], df_up1[\"customer_id\"], df_up1[\"pizza_id\"],df_up1[\"extras_array\"],df_up1[\"exclusions_array\"],df4[\"toppings\"])\n",
    "df_co.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "503e6830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+------------+----------------+--------------------+--------------------+\n",
      "|order_id|customer_id|pizza_id|extras_array|exclusions_array|            toppings|            New_Topp|\n",
      "+--------+-----------+--------+------------+----------------+--------------------+--------------------+\n",
      "|       1|        101|       1|          []|              []|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       2|        101|       1|          []|              []|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       3|        102|       1|          []|              []|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       4|        103|       1|          []|             [4]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       4|        103|       1|          []|             [4]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       5|        104|       1|         [1]|          [null]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       8|        102|       1|      [null]|          [null]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       9|        103|       1|     [1,  5]|             [4]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|      10|        104|       1|      [null]|          [null]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|      10|        104|       1|     [1,  4]|         [2,  6]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       3|        102|       2|      [null]|              []|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|\n",
      "|       4|        103|       2|          []|             [4]|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|\n",
      "|       6|        101|       2|      [null]|          [null]|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|\n",
      "|       7|        105|       2|         [1]|          [null]|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|\n",
      "+--------+-----------+--------+------------+----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df_co = df_co.withColumn(\"New_Topp\", split(df_co.toppings, \",\"))\n",
    "df_co.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a4470cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+------------+----------------+--------------------+--------------------+----------+\n",
      "|order_id|customer_id|pizza_id|extras_array|exclusions_array|            toppings|            New_Topp|pizza_name|\n",
      "+--------+-----------+--------+------------+----------------+--------------------+--------------------+----------+\n",
      "|       1|        101|       1|          []|              []|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...| Meatlover|\n",
      "|       2|        101|       1|          []|              []|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...| Meatlover|\n",
      "|       3|        102|       1|          []|              []|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...| Meatlover|\n",
      "|       4|        103|       1|          []|             [4]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...| Meatlover|\n",
      "|       4|        103|       1|          []|             [4]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...| Meatlover|\n",
      "|       5|        104|       1|         [1]|          [null]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...| Meatlover|\n",
      "|       8|        102|       1|      [null]|          [null]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...| Meatlover|\n",
      "|       9|        103|       1|     [1,  5]|             [4]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...| Meatlover|\n",
      "|      10|        104|       1|      [null]|          [null]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...| Meatlover|\n",
      "|      10|        104|       1|     [1,  4]|         [2,  6]|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...| Meatlover|\n",
      "|       3|        102|       2|      [null]|              []|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|Vegetarian|\n",
      "|       4|        103|       2|          []|             [4]|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|Vegetarian|\n",
      "|       6|        101|       2|      [null]|          [null]|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|Vegetarian|\n",
      "|       7|        105|       2|         [1]|          [null]|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|Vegetarian|\n",
      "+--------+-----------+--------+------------+----------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_co = df_co.withColumn(\"pizza_name\", \n",
    "                   when(df_co.pizza_id == 1, \"Meatlover\").otherwise(\"Vegetarian\"))\n",
    "\n",
    "df_co.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b1a7de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+--------------------+\n",
      "|order_id|customer_id|pizza_id|pizza_name|        all_toppings|\n",
      "+--------+-----------+--------+----------+--------------------+\n",
      "|      10|        104|       1| Meatlover|[Mushrooms, BBQ S...|\n",
      "|       4|        103|       2|Vegetarian|            [Cheese]|\n",
      "|       4|        103|       1| Meatlover|    [Cheese, Cheese]|\n",
      "|       9|        103|       1| Meatlover|            [Cheese]|\n",
      "+--------+-----------+--------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "\n",
    "NewDF_exploded2 = df_co.select(\"order_id\",\"customer_id\",\"pizza_id\", \"pizza_name\", explode(\"exclusions_array\").alias(\"topping_id\"))\n",
    "\n",
    "Output2 = NewDF_exploded2.join(df5, on=\"topping_id\")\n",
    "\n",
    "AF2 = Output2.groupBy(\"order_id\",\"customer_id\",\"pizza_id\", \"pizza_name\").agg(collect_list(\"topping_name\").alias(\"all_toppings\"))\n",
    "AF2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "593ed7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+--------------------+--------------------+\n",
      "|order_id|customer_id|pizza_id|pizza_name|        all_toppings|      new_pizza_name|\n",
      "+--------+-----------+--------+----------+--------------------+--------------------+\n",
      "|      10|        104|       1| Meatlover|[Mushrooms, BBQ S...|           Meatlover|\n",
      "|       4|        103|       2|Vegetarian|            [Cheese]|Vegetarian_exclud...|\n",
      "|       4|        103|       1| Meatlover|    [Cheese, Cheese]|Meatlover_exclude...|\n",
      "|       9|        103|       1| Meatlover|            [Cheese]|Meatlover_exclude...|\n",
      "+--------+-----------+--------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, when, col, array_contains\n",
    "\n",
    "# assume df is the DataFrame containing the columns to concatenate\n",
    "AF2 = AF2.withColumn(\"new_pizza_name\",\n",
    "                   when(array_contains(col(\"all_toppings\"), \"Cheese\"),\n",
    "                        concat(col(\"pizza_name\"), lit(\"_exclude_cheese\"))).otherwise(col(\"pizza_name\")))\n",
    "\n",
    "AF2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f766f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+----------------+\n",
      "|order_id|customer_id|pizza_id|pizza_name|    all_toppings|\n",
      "+--------+-----------+--------+----------+----------------+\n",
      "|      10|        104|       1| Meatlover| [Bacon, Cheese]|\n",
      "|       5|        104|       1| Meatlover|         [Bacon]|\n",
      "|       7|        105|       2|Vegetarian|         [Bacon]|\n",
      "|       9|        103|       1| Meatlover|[Chicken, Bacon]|\n",
      "+--------+-----------+--------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "\n",
    "NewDF_exploded1 = df_co.select(\"order_id\",\"customer_id\",\"pizza_id\", \"pizza_name\", explode(\"extras_array\").alias(\"topping_id\"))\n",
    "\n",
    "Output1 = NewDF_exploded1.join(df5, on=\"topping_id\")\n",
    "\n",
    "AF1 = Output1.groupBy(\"order_id\",\"customer_id\",\"pizza_id\", \"pizza_name\").agg(collect_list(\"topping_name\").alias(\"all_toppings\"))\n",
    "AF1.show()\n",
    "\n",
    "# Meat Lovers\n",
    "# Meat Lovers - Exclude Beef\n",
    "# Meat Lovers - Extra Bacon\n",
    "# Meat Lovers - Exclude Cheese, Bacon - Extra Mushroom, Peppers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f26e85c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+----------------+--------------------+\n",
      "|order_id|customer_id|pizza_id|pizza_name|    all_toppings|      new_pizza_name|\n",
      "+--------+-----------+--------+----------+----------------+--------------------+\n",
      "|      10|        104|       1| Meatlover| [Bacon, Cheese]|Meatlover_extra_B...|\n",
      "|       5|        104|       1| Meatlover|         [Bacon]|Meatlover_extra_B...|\n",
      "|       7|        105|       2|Vegetarian|         [Bacon]|Vegetarian_extra_...|\n",
      "|       9|        103|       1| Meatlover|[Chicken, Bacon]|Meatlover_extra_B...|\n",
      "+--------+-----------+--------+----------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, when, col, array_contains\n",
    "\n",
    "# assume df is the DataFrame containing the columns to concatenate\n",
    "AF1 = AF1.withColumn(\"new_pizza_name\",\n",
    "                   when(array_contains(col(\"all_toppings\"), \"Bacon\"),\n",
    "                        concat(col(\"pizza_name\"), lit(\"_extra_Bacon_\"))).otherwise(col(\"pizza_name\")))\n",
    "\n",
    "AF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b9ab6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+--------------------+--------------------+\n",
      "|order_id|customer_id|pizza_id|pizza_name|        all_toppings|      new_pizza_name|\n",
      "+--------+-----------+--------+----------+--------------------+--------------------+\n",
      "|      10|        104|       1| Meatlover|[Mushrooms, BBQ S...|           Meatlover|\n",
      "|       4|        103|       2|Vegetarian|            [Cheese]|Vegetarian_exclud...|\n",
      "|       4|        103|       1| Meatlover|    [Cheese, Cheese]|Meatlover_exclude...|\n",
      "|       9|        103|       1| Meatlover|            [Cheese]|Meatlover_exclude...|\n",
      "|      10|        104|       1| Meatlover|     [Bacon, Cheese]|Meatlover_extra_B...|\n",
      "|       5|        104|       1| Meatlover|             [Bacon]|Meatlover_extra_B...|\n",
      "|       7|        105|       2|Vegetarian|             [Bacon]|Vegetarian_extra_...|\n",
      "|       9|        103|       1| Meatlover|    [Chicken, Bacon]|Meatlover_extra_B...|\n",
      "+--------+-----------+--------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_df =AF2.union(AF1)\n",
    "union_df.show()\n",
    "\n",
    "# Meat Lovers\n",
    "# Meat Lovers - Exclude Beef\n",
    "# Meat Lovers - Extra Bacon\n",
    "# Meat Lovers - Exclude Cheese, Bacon - Extra Mushroom, Peppers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b165a9",
   "metadata": {},
   "source": [
    "# 5. Generate an alphabetically ordered comma separated ingredient list for each pizza order from the customer_orders table and add a 2x in front of any relevant ingredients\n",
    "For example: \"Meat Lovers: 2xBacon, Beef, ... , Salami\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "255d8fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+----------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|pizza_name|\n",
      "+--------+-----------+--------+----------+------+-------------------+----------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|Meatlovers|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|Meatlovers|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|Meatlovers|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|Meatlovers|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|Meatlovers|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|Meatlovers|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|Meatlovers|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|Meatlovers|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|Meatlovers|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|Meatlovers|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|Vegetarian|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|Vegetarian|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|Vegetarian|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|Vegetarian|\n",
      "+--------+-----------+--------+----------+------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "New_Alpha = df1.join(df3, df1.pizza_id == df3.pizza_id, how = 'inner').select(df1[\"*\"], df3[\"pizza_name\"])\n",
    "New_Alpha.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b84b3e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+----------+--------------------+--------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|pizza_name|            toppings|            New_Topp|\n",
      "+--------+-----------+--------+----------+------+-------------------+----------+--------------------+--------------------+\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|Meatlovers|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|Meatlovers|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|Meatlovers|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|Meatlovers|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|Meatlovers|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|Meatlovers|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|Meatlovers|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|Meatlovers|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|Meatlovers|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|Meatlovers|1, 2, 3, 4, 5, 6,...|[1,  2,  3,  4,  ...|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|Vegetarian|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|Vegetarian|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|Vegetarian|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|Vegetarian|  4, 6, 7, 9, 11, 12|[4,  6,  7,  9,  ...|\n",
      "+--------+-----------+--------+----------+------+-------------------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "New_Alpha1 = New_Alpha.join(df4, New_Alpha.pizza_id == df4.pizza_id, how = 'inner').select(New_Alpha[\"*\"],df4[\"toppings\"])\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "New_Alpha1 = New_Alpha1.withColumn(\"New_Topp\", split(NewDF.toppings, \",\"))\n",
    "New_Alpha1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aacd2e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------------------+\n",
      "|order_id|pizza_id|pizza_name|        all_toppings|\n",
      "+--------+--------+----------+--------------------+\n",
      "|       9|       1|Meatlovers|[Mushrooms, Chick...|\n",
      "|       1|       1|Meatlovers|[Mushrooms, Chick...|\n",
      "|       4|       2|Vegetarian|[Onions, Mushroom...|\n",
      "|       6|       2|Vegetarian|[Onions, Mushroom...|\n",
      "|      10|       1|Meatlovers|[Mushrooms, Mushr...|\n",
      "|       3|       1|Meatlovers|[Mushrooms, Chick...|\n",
      "|       4|       1|Meatlovers|[Mushrooms, Mushr...|\n",
      "|       2|       1|Meatlovers|[Mushrooms, Chick...|\n",
      "|       3|       2|Vegetarian|[Onions, Mushroom...|\n",
      "|       8|       1|Meatlovers|[Mushrooms, Chick...|\n",
      "|       7|       2|Vegetarian|[Onions, Mushroom...|\n",
      "|       5|       1|Meatlovers|[Mushrooms, Chick...|\n",
      "+--------+--------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "\n",
    "New_Alpha2 = New_Alpha1.select(\"order_id\", \"pizza_id\",\"pizza_name\", explode(\"New_Topp\").alias(\"topping_id\"))\n",
    "\n",
    "\n",
    "New_Alpha3 = New_Alpha2.join(df5, on=\"topping_id\")\n",
    "\n",
    "\n",
    "New_Alpha4 = New_Alpha3.groupBy(\"order_id\",\"pizza_id\",\"pizza_name\").agg(collect_list(\"topping_name\").alias(\"all_toppings\"))\n",
    "New_Alpha4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e7aebbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------------------+--------------------+\n",
      "|order_id|pizza_id|pizza_name|        all_toppings| pizza_with_toppings|\n",
      "+--------+--------+----------+--------------------+--------------------+\n",
      "|       1|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: BBQ S...|\n",
      "|       2|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: BBQ S...|\n",
      "|       3|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: BBQ S...|\n",
      "|       3|       2|Vegetarian|[Onions, Mushroom...|Vegetarian: Chees...|\n",
      "|       4|       2|Vegetarian|[Onions, Mushroom...|Vegetarian: Chees...|\n",
      "|       4|       1|Meatlovers|[Mushrooms, Mushr...|Meatlovers: BBQ S...|\n",
      "|       5|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: BBQ S...|\n",
      "|       6|       2|Vegetarian|[Onions, Mushroom...|Vegetarian: Chees...|\n",
      "|       7|       2|Vegetarian|[Onions, Mushroom...|Vegetarian: Chees...|\n",
      "|       8|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: BBQ S...|\n",
      "|       9|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: BBQ S...|\n",
      "|      10|       1|Meatlovers|[Mushrooms, Mushr...|Meatlovers: BBQ S...|\n",
      "+--------+--------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, concat_ws, sort_array\n",
    "\n",
    "# Sort the all_toppings array column in alphabetical order\n",
    "sorted_toppings = sort_array(New_Alpha4.all_toppings)\n",
    "\n",
    "# Concatenate the pizza_name and sorted_toppings columns\n",
    "concatenated_cols = concat_ws(\": \", New_Alpha4.pizza_name, sorted_toppings)\n",
    "\n",
    "# Add the concatenated columns as a new column to the DataFrame\n",
    "New_Alpha5 = New_Alpha4.withColumn(\"pizza_with_toppings\", concatenated_cols)\n",
    "\n",
    "New_Alpha5 = New_Alpha5.orderBy(\"order_id\")\n",
    "\n",
    "# Show the result\n",
    "New_Alpha5.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e271fe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------------------+--------------------+\n",
      "|order_id|pizza_id|pizza_name|        all_toppings| pizza_with_toppings|\n",
      "+--------+--------+----------+--------------------+--------------------+\n",
      "|       1|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: 2x BB...|\n",
      "|       2|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: 2x BB...|\n",
      "|       3|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: 2x BB...|\n",
      "|       3|       2|Vegetarian|[Onions, Mushroom...|Vegetarian: 2x Ch...|\n",
      "|       4|       2|Vegetarian|[Onions, Mushroom...|Vegetarian: 2x Ch...|\n",
      "|       4|       1|Meatlovers|[Mushrooms, Mushr...|Meatlovers: 2x BB...|\n",
      "|       5|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: 2x BB...|\n",
      "|       6|       2|Vegetarian|[Onions, Mushroom...|Vegetarian: 2x Ch...|\n",
      "|       7|       2|Vegetarian|[Onions, Mushroom...|Vegetarian: 2x Ch...|\n",
      "|       8|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: 2x BB...|\n",
      "|       9|       1|Meatlovers|[Mushrooms, Chick...|Meatlovers: 2x BB...|\n",
      "|      10|       1|Meatlovers|[Mushrooms, Mushr...|Meatlovers: 2x BB...|\n",
      "+--------+--------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "\n",
    "New_Alpha5 = New_Alpha5.withColumn('pizza_with_toppings', \n",
    "                   regexp_replace('pizza_with_toppings', \n",
    "                                   'BBQ', \n",
    "                                   '2x BBQ'))\n",
    "New_Alpha5 = New_Alpha5.withColumn('pizza_with_toppings', \n",
    "                   regexp_replace('pizza_with_toppings', \n",
    "                                   'Cheese', \n",
    "                                   '2x Cheese'))\n",
    "\n",
    "\n",
    "New_Alpha5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3da6ac",
   "metadata": {},
   "source": [
    "### 6. What is the total quantity of each ingredient used in all delivered pizzas sorted by most frequent first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6b2e9dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|\n",
      "+--------+-----------+--------+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4e9f75cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|toppings_name|total_quantity|\n",
      "+-------------+--------------+\n",
      "|    Mushrooms|            14|\n",
      "|       Cheese|            14|\n",
      "|         Beef|            10|\n",
      "|      Chicken|            10|\n",
      "|        Bacon|            10|\n",
      "|    Pepperoni|            10|\n",
      "|    BBQ Sauce|            10|\n",
      "|       Salami|            10|\n",
      "|       Onions|             4|\n",
      "|     Tomatoes|             4|\n",
      "|      Peppers|             4|\n",
      "| Tomato Sauce|             4|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, count\n",
    "\n",
    "ingredient_counts = New_Alpha4.select(explode(\"all_toppings\").alias(\"toppings_name\")) \\\n",
    "                                     .groupBy(\"toppings_name\") \\\n",
    "                                     .agg(count(\"*\").alias(\"total_quantity\")) \\\n",
    "                                     .orderBy(\"total_quantity\", ascending=False)\n",
    "\n",
    "ingredient_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd6254f",
   "metadata": {},
   "source": [
    "# D. Pricing and Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a1c7e",
   "metadata": {},
   "source": [
    "###### 1. If a Meat Lovers pizza costs  $12 and Vegetarian costs $10 and there were no charges for changes - how much money has Pizza Runner made so far if there are no delivery fees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "26f4a20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+\n",
      "|pizza_id|pizza_name|pizza_cost|\n",
      "+--------+----------+----------+\n",
      "|       1|Meatlovers|        12|\n",
      "|       2|Vegetarian|        10|\n",
      "+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, when, col\n",
    "\n",
    "T_Price  = df3.select(\"pizza_id\", \"pizza_name\", when(col(\"pizza_name\") == \"Meatlovers\", 12).otherwise(10).alias(\"pizza_cost\"))\n",
    "T_Price.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "29df0399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+---------+--------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|runner_id|        cancellation|\n",
      "+--------+-----------+--------+----------+------+-------------------+---------+--------------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|        1|                    |\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|        1|                    |\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|        1|                null|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|        1|                null|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|        2|                null|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|        2|                null|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|        2|                null|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|        3|                null|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|        3|Restaurant Cancel...|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|        2|                null|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|        2|                null|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|        2|Customer Cancella...|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|        1|                null|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|        1|                null|\n",
      "+--------+-----------+--------+----------+------+-------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T_Price2 = df1.join(df2, df1.order_id == df2.order_id, how = 'inner').select(df1[\"*\"],df2[\"runner_id\"], df2[\"cancellation\"])\n",
    "T_Price2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4375e7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+---------+--------------------+----------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|runner_id|        cancellation|pizza_cost|\n",
      "+--------+-----------+--------+----------+------+-------------------+---------+--------------------+----------+\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|        1|                null|        12|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|        1|                null|        12|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|        2|Customer Cancella...|        12|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|        2|                null|        12|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|        3|                null|        12|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|        2|                null|        12|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|        2|                null|        12|\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|        1|                null|        12|\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|        1|                    |        12|\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|        1|                    |        12|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|        2|                null|        10|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|        3|Restaurant Cancel...|        10|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|        2|                null|        10|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|        1|                null|        10|\n",
      "+--------+-----------+--------+----------+------+-------------------+---------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T_Price3 = T_Price2.join(T_Price, T_Price2.pizza_id == T_Price.pizza_id, how = 'inner').select(T_Price2[\"*\"],T_Price[\"pizza_cost\"])\n",
    "\n",
    "T_Price3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6866682b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|runner_id|total_revenue|\n",
      "+---------+-------------+\n",
      "|        3|           22|\n",
      "|        1|           70|\n",
      "|        2|           68|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_revenue_df = T_Price3\\\n",
    "                    .groupBy(\"runner_id\")\\\n",
    "                    .agg(sum(col(\"pizza_cost\")).alias(\"total_revenue\"))\\\n",
    "                   \n",
    "\n",
    "total_revenue_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b3e4a",
   "metadata": {},
   "source": [
    "# 2. What if there was an additional $1 charge for any pizza extras?\n",
    "\n",
    "    Add cheese is $1 extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "662177b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+---------+--------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|runner_id|        cancellation|\n",
      "+--------+-----------+--------+----------+------+-------------------+---------+--------------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|        1|                    |\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|        1|                    |\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|        1|                null|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|        1|                null|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|        2|                null|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|        2|                null|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|        2|                null|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|        3|                null|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|        3|Restaurant Cancel...|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|        2|                null|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|        2|                null|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|        2|Customer Cancella...|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|        1|                null|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|        1|                null|\n",
      "+--------+-----------+--------+----------+------+-------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, when, col\n",
    "\n",
    "ADD_Price = df1.join(df2, df1.order_id == df2.order_id, how = 'inner').select(df1[\"*\"],df2[\"runner_id\"], df2[\"cancellation\"])\n",
    "ADD_Price.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fdb130a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n",
      "|pizza_cost|exclusions|extras|\n",
      "+----------+----------+------+\n",
      "|        12|          |      |\n",
      "|        12|          |      |\n",
      "|        12|          |      |\n",
      "|        10|          |  null|\n",
      "|        12|         4|      |\n",
      "|        12|         4|      |\n",
      "|        10|         4|      |\n",
      "|        12|      null|     1|\n",
      "|        10|      null|  null|\n",
      "|        10|      null|     1|\n",
      "|        12|      null|  null|\n",
      "|        12|         4|  1, 5|\n",
      "|        12|      null|  null|\n",
      "|        12|      2, 6|  1, 4|\n",
      "+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ADD_Price1  = ADD_Price \\\n",
    "          .select(when(col(\"pizza_id\") == \"1\", 12).otherwise(10).alias(\"pizza_cost\"), \"exclusions\", \"extras\")\\\n",
    "\n",
    "ADD_Price1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "81eab97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|total_earn|\n",
      "+----------+\n",
      "|       186|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ADD_revenue_df = ADD_Price1.select(\n",
    "  sum(\n",
    "    when(col(\"extras\").isNull(), col(\"pizza_cost\"))\n",
    "      .when(length(col(\"extras\")) == 1, col(\"pizza_cost\") + 1)\n",
    "      .otherwise(col(\"pizza_cost\") + 2)\n",
    "  ).alias(\"total_earn\")\n",
    ")\n",
    "\n",
    "ADD_revenue_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce137d",
   "metadata": {},
   "source": [
    "## 3. The Pizza Runner team now wants to add an additional ratings system that allows customers to rate their runner, how would you design an additional table for this new dataset - generate a schema for this new table and insert your own data for ratings for each successful customer order between 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bee83932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|order_id|rating|\n",
      "+--------+------+\n",
      "|       1|     3|\n",
      "|       2|     4|\n",
      "|       3|     5|\n",
      "|       4|     2|\n",
      "|       5|     1|\n",
      "|       6|     3|\n",
      "|       7|     4|\n",
      "|       8|     1|\n",
      "|       9|     3|\n",
      "|      10|     5|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [(1, 3), (2, 4), (3, 5), (4, 2), (5, 1), (6, 3), (7, 4), (8, 1), (9, 3), (10, 5)]\n",
    "schema = StructType([StructField(\"order_id\", IntegerType(), True), StructField(\"rating\", IntegerType(), True)])\n",
    "ratings_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "ratings_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ddafbc",
   "metadata": {},
   "source": [
    "## 4. Using your newly generated table - can you join all of the information together to form a table which has the following information for successful deliveries?\n",
    "    customer_id\n",
    "    order_id\n",
    "    runner_id\n",
    "    rating\n",
    "    order_time\n",
    "    pickup_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4dfc12ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------+------+-------------------+---------+--------+--------+-------------------+--------------------+\n",
      "|order_id|customer_id|pizza_id|exclusions|extras|         order_time|runner_id|duration|distance|        pickup_time|        cancellation|\n",
      "+--------+-----------+--------+----------+------+-------------------+---------+--------+--------+-------------------+--------------------+\n",
      "|       1|        101|       1|          |      |2020-01-01 18:05:02|        1|     32 |      20|2020-01-01 18:15:34|                    |\n",
      "|       2|        101|       1|          |      |2020-01-01 19:00:52|        1|     27 |      20|2020-01-01 19:10:54|                    |\n",
      "|       3|        102|       1|          |      |2020-01-02 23:51:23|        1|     20 |    13.4|2020-01-03 00:12:37|                null|\n",
      "|       3|        102|       2|          |  null|2020-01-02 23:51:23|        1|     20 |    13.4|2020-01-03 00:12:37|                null|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|        2|      40|    23.4|2020-01-04 13:53:03|                null|\n",
      "|       4|        103|       1|         4|      |2020-01-04 13:23:46|        2|      40|    23.4|2020-01-04 13:53:03|                null|\n",
      "|       4|        103|       2|         4|      |2020-01-04 13:23:46|        2|      40|    23.4|2020-01-04 13:53:03|                null|\n",
      "|       5|        104|       1|      null|     1|2020-01-08 21:00:29|        3|      15|      10|2020-01-08 21:10:57|                null|\n",
      "|       6|        101|       2|      null|  null|2020-01-08 21:03:13|        3|    null|    null|               null|Restaurant Cancel...|\n",
      "|       7|        105|       2|      null|     1|2020-01-08 21:20:29|        2|      25|      25|2020-01-08 21:30:45|                null|\n",
      "|       8|        102|       1|      null|  null|2020-01-09 23:54:33|        2|     15 |   23.4 |2020-01-10 00:15:02|                null|\n",
      "|       9|        103|       1|         4|  1, 5|2020-01-10 11:22:59|        2|    null|    null|               null|Customer Cancella...|\n",
      "|      10|        104|       1|      null|  null|2020-01-11 18:34:49|        1|      10|      10|2020-01-11 18:50:20|                null|\n",
      "|      10|        104|       1|      2, 6|  1, 4|2020-01-11 18:34:49|        1|      10|      10|2020-01-11 18:50:20|                null|\n",
      "+--------+-----------+--------+----------+------+-------------------+---------+--------+--------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A_Price = df1.join(df2, df1.order_id == df2.order_id, how = 'inner').select(df1[\"*\"],df2[\"runner_id\"], df2[\"duration\"],df2[\"distance\"],df2[\"pickup_time\"], df2[\"cancellation\"])\n",
    "A_Price.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f1243e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+---------+------+-------------------+-------------------+--------+--------+--------------------+\n",
      "|customer_id|pizza_id|order_id|runner_id|rating|         order_time|        pickup_time|duration|distance|        cancellation|\n",
      "+-----------+--------+--------+---------+------+-------------------+-------------------+--------+--------+--------------------+\n",
      "|        101|       1|       1|        1|     3|2020-01-01 18:05:02|2020-01-01 18:15:34|     32 |      20|                    |\n",
      "|        101|       2|       6|        3|     3|2020-01-08 21:03:13|               null|    null|    null|Restaurant Cancel...|\n",
      "|        102|       2|       3|        1|     5|2020-01-02 23:51:23|2020-01-03 00:12:37|     20 |    13.4|                null|\n",
      "|        102|       1|       3|        1|     5|2020-01-02 23:51:23|2020-01-03 00:12:37|     20 |    13.4|                null|\n",
      "|        104|       1|       5|        3|     1|2020-01-08 21:00:29|2020-01-08 21:10:57|      15|      10|                null|\n",
      "|        103|       1|       9|        2|     3|2020-01-10 11:22:59|               null|    null|    null|Customer Cancella...|\n",
      "|        103|       2|       4|        2|     2|2020-01-04 13:23:46|2020-01-04 13:53:03|      40|    23.4|                null|\n",
      "|        103|       1|       4|        2|     2|2020-01-04 13:23:46|2020-01-04 13:53:03|      40|    23.4|                null|\n",
      "|        103|       1|       4|        2|     2|2020-01-04 13:23:46|2020-01-04 13:53:03|      40|    23.4|                null|\n",
      "|        102|       1|       8|        2|     1|2020-01-09 23:54:33|2020-01-10 00:15:02|     15 |   23.4 |                null|\n",
      "|        105|       2|       7|        2|     4|2020-01-08 21:20:29|2020-01-08 21:30:45|      25|      25|                null|\n",
      "|        104|       1|      10|        1|     5|2020-01-11 18:34:49|2020-01-11 18:50:20|      10|      10|                null|\n",
      "|        104|       1|      10|        1|     5|2020-01-11 18:34:49|2020-01-11 18:50:20|      10|      10|                null|\n",
      "|        101|       1|       2|        1|     4|2020-01-01 19:00:52|2020-01-01 19:10:54|     27 |      20|                    |\n",
      "+-----------+--------+--------+---------+------+-------------------+-------------------+--------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "New_Table = A_Price.join(ratings_df, ratings_df.order_id == A_Price.order_id, how = 'inner').select( A_Price[\"customer_id\"],A_Price[\"pizza_id\"] ,A_Price[\"order_id\"],A_Price[\"runner_id\"],ratings_df[\"rating\"],A_Price[\"order_time\"], A_Price[\"pickup_time\"], A_Price[\"duration\"], A_Price[\"distance\"],A_Price[\"cancellation\"])\n",
    "New_Table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a8deafaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------+------+-------------------+-------------------+------------------+--------+---------+-----------+\n",
      "|customer_id|order_id|runner_id|rating|         order_time|        pickup_time|Time__order_pickup|duration|avg_Speed|Pizza_Count|\n",
      "+-----------+--------+---------+------+-------------------+-------------------+------------------+--------+---------+-----------+\n",
      "|        101|       2|        1|     4|2020-01-01 19:00:52|2020-01-01 19:10:54|              null|     27 |    44.44|          1|\n",
      "|        101|       6|        3|     3|2020-01-08 21:03:13|               null|              null|    null|     null|          1|\n",
      "|        101|       1|        1|     3|2020-01-01 18:05:02|2020-01-01 18:15:34|              null|     32 |     37.5|          1|\n",
      "|        102|       3|        1|     5|2020-01-02 23:51:23|2020-01-03 00:12:37|              null|     20 |     40.2|          2|\n",
      "|        102|       8|        2|     1|2020-01-09 23:54:33|2020-01-10 00:15:02|              null|     15 |     93.6|          1|\n",
      "|        103|       4|        2|     2|2020-01-04 13:23:46|2020-01-04 13:53:03|              null|      40|     35.1|          3|\n",
      "|        103|       9|        2|     3|2020-01-10 11:22:59|               null|              null|    null|     null|          1|\n",
      "|        104|       5|        3|     1|2020-01-08 21:00:29|2020-01-08 21:10:57|              null|      15|     40.0|          1|\n",
      "|        104|      10|        1|     5|2020-01-11 18:34:49|2020-01-11 18:50:20|              null|      10|     60.0|          2|\n",
      "|        105|       7|        2|     4|2020-01-08 21:20:29|2020-01-08 21:30:45|              null|      25|     60.0|          1|\n",
      "+-----------+--------+---------+------+-------------------+-------------------+------------------+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Final_Table1 =New_Table\\\n",
    "                .select(\"customer_id\", \"order_id\", \"runner_id\", \"rating\", \"order_time\",\n",
    "                                \"pickup_time\", ((col(\"pickup_time\").cast(\"long\") - col(\"order_time\").cast(\"long\")) / 60).alias(\"Time__order_pickup\"), \n",
    "                                \"duration\", \"distance\", \"pizza_id\")\n",
    "\n",
    "\n",
    "Final_result = Final_Table1.groupBy(\"customer_id\", \"order_id\", \"runner_id\", \"rating\", \"order_time\", \"pickup_time\", \"Time__order_pickup\", \"duration\") \\\n",
    "               .agg(round(avg(col(\"distance\") / col(\"duration\") * 60), 2).alias(\"avg_Speed\"),\n",
    "                    count(when(col(\"pizza_id\").isNotNull(), 1)).alias(\"Pizza_Count\")) \\\n",
    "               .orderBy(\"customer_id\")\n",
    "\n",
    "Final_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d00fec",
   "metadata": {},
   "source": [
    "# 5. If a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre traveled - how much money does Pizza Runner have left over after these deliveries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6e965d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|order_id|pizza_cost|\n",
      "+--------+----------+\n",
      "|       1|        12|\n",
      "|       6|        10|\n",
      "|       3|        22|\n",
      "|       5|        12|\n",
      "|       9|        12|\n",
      "|       4|        34|\n",
      "|       8|        12|\n",
      "|       7|        10|\n",
      "|      10|        24|\n",
      "|       2|        12|\n",
      "+--------+----------+\n",
      "\n",
      "+-------+----------+------+\n",
      "|revenue|total_cost|profit|\n",
      "+-------+----------+------+\n",
      "|    160|     43.56|116.44|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, sum\n",
    "\n",
    "final_cte = df3.join(df1, df1.pizza_id == df3.pizza_id, how = 'inner').select(df1[\"*\"],df3[\"pizza_name\"])\\\n",
    "       .groupBy(\"order_id\")\\\n",
    "       .agg(sum(when(df3.pizza_name == \"Meatlovers\", 12).otherwise(10)).alias(\"pizza_cost\"))\\\n",
    "\n",
    "final_cte.show()\n",
    "      \n",
    "\n",
    "F_result = df2.join(final_cte, df2.order_id == final_cte.order_id, how = 'inner')\\\n",
    "          .agg(sum(final_cte.pizza_cost).alias(\"revenue\"), \n",
    "               sum((df2.distance) * 0.3).alias(\"total_cost\"), \n",
    "               (sum(final_cte.pizza_cost) - sum(df2.distance) * 0.3).alias(\"profit\"))\\\n",
    "         \n",
    "\n",
    "F_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0346a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
